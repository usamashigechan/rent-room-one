import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from datetime import datetime

def scrape_kanto_stations():
    """
    é–¢æ±å…¨åŸŸã®éƒ½çœŒã€è·¯ç·šã€é§…ã”ã¨ã®ç‰©ä»¶æ•°ã®ã¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    # é–¢æ±å„çœŒã®è·¯ç·šãƒšãƒ¼ã‚¸URL
    prefectures = {
        "æ±äº¬éƒ½": "https://suumo.jp/chintai/tokyo/ensen/",
        "ç¥å¥ˆå·çœŒ": "https://suumo.jp/chintai/kanagawa/ensen/",
        "åŸ¼ç‰çœŒ": "https://suumo.jp/chintai/saitama/ensen/",
        "åƒè‘‰çœŒ": "https://suumo.jp/chintai/chiba/ensen/",
        "ç¾¤é¦¬çœŒ": "https://suumo.jp/chintai/gumma/ensen/",
        "èŒ¨åŸçœŒ": "https://suumo.jp/chintai/ibaraki/ensen/",
        "æ ƒæœ¨çœŒ": "https://suumo.jp/chintai/tochigi/ensen/"
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    all_stations = []
    
    print("é–¢æ±å…¨åŸŸã®é§…åˆ¥ç‰©ä»¶æ•°ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    print("=" * 50)
    
    for pref_name, pref_url in prefectures.items():
        print(f"\n{pref_name} å‡¦ç†ä¸­...")
        
        try:
            # éƒ½é“åºœçœŒã®è·¯ç·šãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = requests.get(pref_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è·¯ç·šãƒªãƒ³ã‚¯ã‚’å–å¾—
            route_links = get_route_links(soup)
            print(f"  {len(route_links)}è·¯ç·šã‚’ç™ºè¦‹")
            
            # å„è·¯ç·šã®é§…ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            for i, (route_name, route_url) in enumerate(route_links, 1):
                print(f"  [{i:2d}] {route_name:30s}", end=" ")
                
                stations = get_stations_from_route(route_url, route_name, pref_name, headers)
                
                if stations:
                    all_stations.extend(stations)
                    print(f"â†’ {len(stations)}é§…")
                else:
                    print("â†’ 0é§…")
                
                time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
            print(f"  âœ… {pref_name} å®Œäº†")
            
        except Exception as e:
            print(f"  âŒ {pref_name} ã‚¨ãƒ©ãƒ¼: {e}")
        
        time.sleep(2)  # éƒ½é“åºœçœŒé–“ã®é–“éš”
    
    return all_stations

def get_route_links(soup):
    """è·¯ç·šãƒšãƒ¼ã‚¸ã‹ã‚‰è·¯ç·šãƒªãƒ³ã‚¯ã‚’å–å¾—"""
    route_links = []
    
    # è¤‡æ•°ã®æ–¹æ³•ã§è·¯ç·šãƒªãƒ³ã‚¯ã‚’æ¢ã™
    selectors = [
        "li.searchitem a",
        "div.searchitem a",
        "a[href*='/ensen/']"
    ]
    
    for selector in selectors:
        links = soup.select(selector)
        
        for link in links:
            href = link.get('href', '')
            text = link.text.strip()
            
            # æœ‰åŠ¹ãªè·¯ç·šãƒªãƒ³ã‚¯ã‹ãƒã‚§ãƒƒã‚¯
            if (href and '/ensen/' in href and 
                text and len(text) >= 3 and len(text) <= 30 and
                not any(word in text for word in ['è·¯ç·š', 'ãƒšãƒ¼ã‚¸', 'SUUMO', 'æ¤œç´¢', 'ãƒ˜ãƒ«ãƒ—'])):
                
                # å®Œå…¨URLã‚’ä½œæˆ
                if href.startswith('/'):
                    href = f"https://suumo.jp{href}"
                
                route_links.append((text, href))
        
        if route_links:
            break
    
    # é‡è¤‡é™¤å»
    return list(set(route_links))

def get_stations_from_route(route_url, route_name, pref_name, headers):
    """è·¯ç·šãƒšãƒ¼ã‚¸ã‹ã‚‰é§…ã¨ç‰©ä»¶æ•°ã‚’å–å¾—"""
    try:
        response = requests.get(route_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        stations = []
        
        # æ–¹æ³•1: æ¨™æº–çš„ãªsearchitemæ§‹é€ 
        items = soup.select("li.searchitem")
        for item in items:
            link = item.select_one("a")
            count_span = item.select_one("span.searchitem-list-value")
            
            if link and count_span:
                station_name = clean_station_name(link.text.strip())
                count = count_span.text.strip()
                
                if is_valid_station_name(station_name):
                    stations.append({
                        'éƒ½çœŒå': pref_name,
                        'è·¯ç·šå': route_name,
                        'é§…å': station_name,
                        'ç‰©ä»¶æ•°': count
                    })
        
        # æ–¹æ³•2: ç‰©ä»¶æ•°ã‹ã‚‰é€†ç®—ï¼ˆæ–¹æ³•1ã§å–å¾—ã§ããªã„å ´åˆï¼‰
        if not stations:
            count_spans = soup.select("span.searchitem-list-value")
            for span in count_spans:
                count = span.text.strip()
                
                # è¦ªè¦ç´ ã‚’è¾¿ã£ã¦é§…åãƒªãƒ³ã‚¯ã‚’æ¢ã™
                current = span.parent
                for _ in range(3):
                    if current is None:
                        break
                    
                    link = current.select_one("a")
                    if link and ('/eki/' in link.get('href', '') or 'eki' in link.get('href', '')):
                        station_name = clean_station_name(link.text.strip())
                        
                        if is_valid_station_name(station_name):
                            stations.append({
                                'éƒ½çœŒå': pref_name,
                                'è·¯ç·šå': route_name,
                                'é§…å': station_name,
                                'ç‰©ä»¶æ•°': count
                            })
                        break
                    
                    current = current.parent
        
        return stations
        
    except Exception:
        return []

def clean_station_name(name):
    """é§…åã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    if not name:
        return ""
    
    # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
    name = re.sub(r'[\[\]ï¼ˆï¼‰\(\)]', '', name)
    name = re.sub(r'é§…$', '', name)  # æœ«å°¾ã®ã€Œé§…ã€ã‚’é™¤å»
    name = name.strip()
    
    return name

def is_valid_station_name(name):
    """æœ‰åŠ¹ãªé§…åã‹ãƒã‚§ãƒƒã‚¯"""
    if not name or len(name) < 2 or len(name) > 15:
        return False
    
    invalid_keywords = [
        'è·¯ç·š', 'ãƒšãƒ¼ã‚¸', 'ä¸€è¦§', 'æ¤œç´¢', 'SUUMO', 'ã‚¨ãƒªã‚¢',
        'è©³ç´°', 'æ¡ä»¶', 'ã‚‚ã£ã¨', 'ãƒ˜ãƒ«ãƒ—', 'åœ°å›³'
    ]
    
    return not any(keyword in name for keyword in invalid_keywords)

def save_to_csv(stations_data):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    if not stations_data:
        print("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # DataFrameã«å¤‰æ›
    df = pd.DataFrame(stations_data)
    
    # é‡è¤‡é™¤å»
    df = df.drop_duplicates(subset=['éƒ½çœŒå', 'è·¯ç·šå', 'é§…å'])
    
    # CSVä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'é–¢æ±é§…åˆ¥ç‰©ä»¶æ•°_{timestamp}.csv'
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ’¾ CSVä¿å­˜å®Œäº†: {filename}")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}é§…")
    
    return df, filename

def show_results(df):
    """çµæœã‚’è¡¨ç¤º"""
    print(f"\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ« (å…ˆé ­10è¡Œ):")
    print(df.head(10).to_string(index=False))
    
    print(f"\nğŸ—¾ éƒ½çœŒåˆ¥é§…æ•°:")
    for pref, count in df['éƒ½çœŒå'].value_counts().items():
        print(f"   {pref}: {count}é§…")
    
    # ç‰©ä»¶æ•°ã®å¤šã„é§…ãƒˆãƒƒãƒ—10
    try:
        df_copy = df.copy()
        df_copy['ç‰©ä»¶æ•°_æ•°å€¤'] = df_copy['ç‰©ä»¶æ•°'].str.replace(',', '').str.replace('ä»¶', '').astype(int, errors='ignore')
        top_stations = df_copy.nlargest(10, 'ç‰©ä»¶æ•°_æ•°å€¤')
        
        print(f"\nğŸ† ç‰©ä»¶æ•°ãƒˆãƒƒãƒ—10é§…:")
        for i, (_, row) in enumerate(top_stations.iterrows(), 1):
            print(f"   {i:2d}. {row['éƒ½çœŒå']} {row['è·¯ç·šå']} {row['é§…å']} ({row['ç‰©ä»¶æ•°']})")
    except:
        pass

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    print("ğŸš† é–¢æ±å…¨åŸŸ é§…åˆ¥ç‰©ä»¶æ•°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    print("å‡ºåŠ›: éƒ½çœŒå, è·¯ç·šå, é§…å, ç‰©ä»¶æ•°")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    print("\nğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
    stations_data = scrape_kanto_stations()
    
    if stations_data:
        print(f"\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(stations_data)}é§…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        
        # CSVä¿å­˜
        df, filename = save_to_csv(stations_data)
        
        # çµæœè¡¨ç¤º
        show_results(df)
        
        print(f"\nğŸ‰ å‡¦ç†å®Œäº†! '{filename}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        sample_data = [
            {'éƒ½çœŒå': 'æ±äº¬éƒ½', 'è·¯ç·šå': 'JRå±±æ‰‹ç·š', 'é§…å': 'æ–°å®¿', 'ç‰©ä»¶æ•°': '12,450'},
            {'éƒ½çœŒå': 'æ±äº¬éƒ½', 'è·¯ç·šå': 'JRå±±æ‰‹ç·š', 'é§…å': 'æ¸‹è°·', 'ç‰©ä»¶æ•°': '10,230'},
            {'éƒ½çœŒå': 'ç¥å¥ˆå·çœŒ', 'è·¯ç·šå': 'æ±æ€¥å¤§äº•ç”ºç·š', 'é§…å': 'äºŒå­æ–°åœ°', 'ç‰©ä»¶æ•°': '4,300'},
            {'éƒ½çœŒå': 'ç¥å¥ˆå·çœŒ', 'è·¯ç·šå': 'JRæ±æµ·é“æœ¬ç·š', 'é§…å': 'æ¨ªæµœ', 'ç‰©ä»¶æ•°': '15,230'}
        ]
        
        df = pd.DataFrame(sample_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'é–¢æ±é§…åˆ¥ç‰©ä»¶æ•°_sample_{timestamp}.csv'
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {filename}")
        print(df.to_string(index=False))